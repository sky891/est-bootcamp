from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import os

# âœ… ëœë¤ ì‹œë“œ ê³ ì •
np.random.seed(42)
tf.random.set_seed(42)

def train_t_lstm_time_decay():
    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    DATA_PATH = os.path.join(BASE_DIR, "data", "data_resampled.csv")
    
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {DATA_PATH}\nğŸ“Œ `preprocessing.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    
    # âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    df = pd.read_csv(DATA_PATH)
    print(f"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {DATA_PATH}")
    print(df.info())

    # âœ… ì‹œê°„ ë³€í™˜ ë° ì •ë ¬
    df['time'] = pd.to_datetime(df['time'])  # ğŸ”¹ ë¬¸ìì—´ â†’ datetime ë³€í™˜
    df['time_decay'] = df['time'].diff().dt.total_seconds()  # ğŸ”¹ ì‹œê°„ ì°¨ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
    df['time_decay'] = np.exp(-df['time_decay'].fillna(0) / 100000)  # ğŸ”¹ ê°ì‡  ê³„ì‚°

    # âœ… ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    columns_to_scale = ['latitude', 'longitude', 'depth', 'rms', 'time_decay']
    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

    # âœ… Feature ì„ íƒ
    features = ['latitude', 'longitude', 'depth', 'rms', 'time_decay']
    target = 'mag'

    X = df[features]
    y = df[target]
    time_decay = df['time_decay']
    timesteps = 30

    # âœ… ë°ì´í„° ë¶„í• 
    train_size = int(len(df) * 0.8)
    val_size = int(len(df) * 0.1)

    # âœ… ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
    def create_sequences(X, y, time_decay, timesteps):
        X_seq = np.array([X.iloc[i:i+timesteps].values for i in range(len(X) - timesteps)], dtype=np.float32)
        y_seq = np.array([y.iloc[i+timesteps] for i in range(len(y) - timesteps)], dtype=np.float32)
        time_decay_seq = np.array([time_decay.iloc[i:i+timesteps].values for i in range(len(y) - timesteps)], dtype=np.float32).reshape(-1, timesteps, 1)
        return X_seq, y_seq, time_decay_seq

    # âœ… í›ˆë ¨, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    X_train, y_train, time_decay_train = create_sequences(X[:train_size], y[:train_size], time_decay[:train_size], timesteps)
    X_val, y_val, time_decay_val = create_sequences(X[train_size:train_size + val_size], y[train_size:train_size + val_size], time_decay[train_size:train_size + val_size], timesteps)
    X_test, y_test, time_decay_test = create_sequences(X[train_size + val_size:], y[train_size + val_size:], time_decay[train_size + val_size:], timesteps)

    # âœ… ë°ì´í„° íƒ€ì… ë³€í™˜ (float32)
    X_train, y_train, time_decay_train = X_train.astype(np.float32), y_train.astype(np.float32), time_decay_train.astype(np.float32)
    X_val, y_val, time_decay_val = X_val.astype(np.float32), y_val.astype(np.float32), time_decay_val.astype(np.float32)
    X_test, y_test, time_decay_test = X_test.astype(np.float32), y_test.astype(np.float32), time_decay_test.astype(np.float32)

    print(f"âœ… ë°ì´í„° í¬ê¸° í™•ì¸: X_train={X_train.shape}, time_decay_train={time_decay_train.shape}, y_train={y_train.shape}")

    # âœ… LSTM ëª¨ë¸ êµ¬ì„±
    X_input = Input(shape=(X_train.shape[1], X_train.shape[2]), name='X_input')
    time_input = Input(shape=(X_train.shape[1], 1), name='time_input')
    merged_input = Lambda(lambda x: x[0] * x[1])([X_input, time_input])

    lstm_out_1 = LSTM(128, return_sequences=True)(merged_input)
    lstm_out_2 = LSTM(64, return_sequences=False)(lstm_out_1)
    output = Dense(1, activation='linear', name='output')(lstm_out_2)

    model = Model(inputs=[X_input, time_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])

    # âœ… í•™ìŠµ ì§„í–‰
    history = model.fit(
        [X_train, time_decay_train], y_train,
        validation_data=([X_val, time_decay_val], y_val),
        epochs=10, batch_size=32,
        verbose=1
    )

    # âœ… ëª¨ë¸ í‰ê°€
    y_pred = model.predict([X_test, time_decay_test])
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"âœ… Test MSE: {mse}, Test MAE: {mae}, Test R2 Score: {r2}")

    # âœ… ì†ì‹¤ ê·¸ë˜í”„
    plt.figure(figsize=(12, 5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('MSE Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    # âœ… ì˜ˆì¸¡ê°’ ê·¸ë˜í”„
    plt.figure(figsize=(12, 5))
    plt.plot(y_test, label='Actual')
    plt.plot(y_pred, label='Predicted')
    plt.xlabel('Samples')
    plt.ylabel('Magnitude')
    plt.title('Actual vs Predicted Magnitude')
    plt.legend()
    plt.show()

    return model