from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from statsmodels.tsa.stattools import adfuller
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dropout, SimpleRNN, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import tensorflow as tf

# âœ… ëœë¤ ì‹œë“œ ê³ ì •
np.random.seed(42)
tf.random.set_seed(42)

def train_rnn():
    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    DATA_PATH = os.path.join(BASE_DIR, "data", "data_resampled.csv")
    
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {DATA_PATH}\nğŸ“Œ `preprocessing.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    
    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    df = pd.read_csv(DATA_PATH)
    print(f"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {DATA_PATH}")
    print(df.info())
    
    # ì‹œê°„ ë³€í™˜ ë° ì •ë ¬
    df = pd.read_csv(DATA_PATH, index_col=0)  # 'time'ì´ ì¸ë±ìŠ¤ë¡œ ë˜ì–´ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    df.reset_index(inplace=True)
    
    # ì»¬ëŸ¼ ë³€í™˜ ë° í‘œì¤€í™”
    scaler = StandardScaler()
    columns_to_scale = ['latitude', 'longitude', 'depth', 'rms']
    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])
    
    # ADF ê²€ì • ì‹¤í–‰ ì „ì— 'mag'ì´ ìƒìˆ˜ì¸ì§€ í™•ì¸
    if df['mag'].nunique() > 1:  # ì„œë¡œ ë‹¤ë¥¸ ê°’ì´ 1ê°œë³´ë‹¤ ë§ì„ ë•Œë§Œ ì‹¤í–‰
        result = adfuller(df['mag'])
        print("ADF Statistic:", result[0])
        print("p-value:", result[1])
    else:
        print("Skipping ADF test: 'mag' column is constant.")
    
    # ë¼ë²¨ ì¸ì½”ë”© ì¶”ê°€ (ì§€ì§„ ë°œìƒ ì—¬ë¶€)
    df['Earthquake'] = df['mag'].apply(lambda x: 'ì§€ì§„ ë°œìƒ' if x > 0 else 'ì§€ì§„ ë¯¸ë°œìƒ')
    df['Earthquake_Label'] = LabelEncoder().fit_transform(df['Earthquake'])
    df.drop('Earthquake', axis=1, inplace=True)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    time_steps = 24
    train_size = int(len(df) * 0.8)
    val_size = int(len(df) * 0.1)
    
    def create_dataset(data, time_steps=24):
        X, y = [], []
        for i in range(len(data) - time_steps):
            X.append(data[i:i + time_steps])
            y.append(data[i + time_steps])
        return np.array(X), np.array(y)
    
    # âœ… time ì»¬ëŸ¼ ì œê±° (ì›ë³¸ Jupyter ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)
    if 'time' in df.columns:
        df.drop(columns=['time'], inplace=True)
    
    X_train, y_train = create_dataset(df[:train_size].values, time_steps)
    X_val, y_val = create_dataset(df[train_size:train_size + val_size].values, time_steps)
    X_test, y_test = create_dataset(df[train_size + val_size:].values, time_steps)
    
    # # # ë°ì´í„° ë³€í™˜ (float32 ì ìš©)
    # # X_train = X_train.astype('float32')
    # # y_train = y_train.astype('float32')
    # # X_val = X_val.astype('float32')
    # # y_val = y_val.astype('float32')
    # # X_test = X_test.astype('float32')
    # # y_test = y_test.astype('float32')

    # # numpy ë°°ì—´ë¡œ ë³€í™˜
    # X_train = np.array(X_train)
    # y_train = np.array(y_train)
    # X_val = np.array(X_val)
    # y_val = np.array(y_val)
    # X_test = np.array(X_test)
    # y_test = np.array(y_test)

    # # RNN ì…ë ¥ ì°¨ì› ë§ì¶”ê¸°
    # X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    # X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))
    # X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
    
    # ìë™ìœ¼ë¡œ feature ê°œìˆ˜ë¥¼ ëª¨ë¸ input_shapeì— ë§ì¶¤
    input_dim = X_train.shape[-1]
     
    
    # RNN ëª¨ë¸ êµ¬ì„± ë° í•™ìŠµ (ì²« ë²ˆì§¸ ëª¨ë¸)
    model = Sequential([
        SimpleRNN(50, activation='tanh', return_sequences=True, input_shape=(time_steps, input_dim), kernel_regularizer=l2(0.03)),
        Dropout(0.3),
        SimpleRNN(50, activation='tanh', kernel_regularizer=l2(0.01)),
        Dropout(0.3),
        Dense(26)
    ])
    
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    
    history = model.fit(X_train, y_train, epochs=15, batch_size=16, validation_data=(X_val, y_val), callbacks=[early_stop])
    
    # ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ ìƒì„±
    data_df = df.copy()
    train_size_c = int(len(data_df) * 0.8)
    val_size_c = int(len(data_df) * 0.1)
    
    X_train_c, y_train_c = create_dataset(data_df[:train_size_c].values, time_steps)
    X_val_c, y_val_c = create_dataset(data_df[train_size_c:train_size_c + val_size].values, time_steps)
    X_test_c, y_test_c = create_dataset(data_df[train_size_c + val_size_c:].values, time_steps)
    
    # ë‘ ë²ˆì§¸ RNN ëª¨ë¸ í•™ìŠµ
    model_rnn_c = Sequential([
        SimpleRNN(50, activation='tanh', return_sequences=True, input_shape=(time_steps, 26), kernel_regularizer=l2(0.03)),
        Dropout(0.3),
        SimpleRNN(50, activation='tanh', kernel_regularizer=l2(0.01)),
        Dropout(0.3),
        Dense(26)
    ])
    
    model_rnn_c.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    early_stop_c = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    
    history_rnn_c = model_rnn_c.fit(X_train_c, y_train_c, epochs=15, batch_size=16, validation_data=(X_val_c, y_val_c), callbacks=[early_stop_c])
    
    # ë‘ ë²ˆì§¸ ëª¨ë¸ í‰ê°€
    loss_rnn_c = model_rnn_c.evaluate(X_test_c, y_test_c, verbose=0)
    print(f'Test Loss (RNN_C): {loss_rnn_c:.2f}')
    
    # í•™ìŠµ ì†ì‹¤ ì‹œê°í™” (ë‘ ë²ˆì§¸ ëª¨ë¸)
    plt.figure(figsize=(10, 6))
    plt.plot(history_rnn_c.history['loss'], label='Training Loss (RNN_C)')
    plt.plot(history_rnn_c.history['val_loss'], label='Validation Loss (RNN_C)')
    plt.xlabel('Epochs')
    plt.ylabel('MSE Loss')
    plt.title('Training and Validation Loss Over Epochs (RNN_C)')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return model, model_rnn_c