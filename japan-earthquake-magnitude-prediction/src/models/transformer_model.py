from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import tensorflow as tf

# âœ… ëœë¤ ì‹œë“œ ê³ ì •
np.random.seed(42)
tf.random.set_seed(42)

def train_transformer():
    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    DATA_PATH = os.path.join(BASE_DIR, "data", "data_resampled.csv")
    
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {DATA_PATH}\nğŸ“Œ `preprocessing.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    
    # âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    df = pd.read_csv(DATA_PATH)
    print(f"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {DATA_PATH}")
    print(df.info())
    
    # âœ… ì‹œê°„ ë³€í™˜ ë° ì •ë ¬
    df['time'] = pd.to_datetime(df['time'])
    df['time'] = df['time'].astype(np.int64) // 10**9 // 3600  # ì´ˆ ë‹¨ìœ„ì—ì„œ ì‹œê°„ ë‹¨ìœ„ë¡œ ë³€í™˜
    
    # âœ… Feature ì„ íƒ
    features = ['latitude', 'longitude', 'depth', 'mag', 'time']
    df = df[features]
    
    # âœ… ë°ì´í„° ë¶„í•  (80% í›ˆë ¨, 10% ê²€ì¦, 10% í…ŒìŠ¤íŠ¸)
    train_size = int(len(df) * 0.8)
    val_size = int(len(df) * 0.1)
    
    X_train = df[:train_size]
    X_val = df[train_size:train_size + val_size]
    X_test = df[train_size + val_size:]
    
    y_train = X_train.pop('mag')
    y_val = X_val.pop('mag')
    y_test = X_test.pop('mag')
    
    print(f"X_train.shape BEFORE: {X_train.shape}")  # (batch_size, feature_dim)

    # âœ… `X_train`ì„ 3Dë¡œ ë³€í™˜ (sequence_length=1 ì¶”ê°€)
    X_train = np.expand_dims(X_train, axis=1)  # (batch_size, feature_dim) â†’ (batch_size, 1, feature_dim)
    X_val = np.expand_dims(X_val, axis=1)
    X_test = np.expand_dims(X_test, axis=1)

    print(f"X_train.shape AFTER: {X_train.shape}")  # (batch_size, 1, feature_dim)

    # âœ… Transformer ëª¨ë¸ êµ¬ì„±
    inputs = Input(shape=(1, X_train.shape[2]))  # ğŸ”¹ sequence_length=1ë¡œ ì„¤ì •
    x = MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)
    x = LayerNormalization()(x)
    x = Dense(64, activation="relu")(x)
    x = Dense(32, activation="relu")(x)
    x = Dropout(0.3)(x)
    x = Flatten()(x)
    outputs = Dense(1)(x)  # ğŸ”¹ ë‹¨ì¼ ì¶œë ¥ìœ¼ë¡œ ë³€ê²½

    model = Model(inputs, outputs)
    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))
    
    # âœ… ëª¨ë¸ í•™ìŠµ
    history = model.fit(X_train, y_train, epochs=15, batch_size=16, validation_data=(X_val, y_val), verbose=1)
    
    # âœ… ëª¨ë¸ í‰ê°€
    loss = model.evaluate(X_test, y_test, verbose=0)
    print(f'Test Loss: {loss:.2f}')
    
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f'Mean Squared Error (MSE): {mse:.2f}')
    print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')
    print(f'Mean Absolute Error (MAE): {mae:.2f}')
    print(f'R-squared (RÂ²): {r2:.2f}')
    
    # âœ… í•™ìŠµ ì†ì‹¤ ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('MSE Loss')
    plt.title('Training and Validation Loss Over Epochs')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return model