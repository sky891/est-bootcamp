from statsmodels.tsa.stattools import adfuller
import contextily as ctx
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import seaborn as sns

# âœ… ë°ì´í„° ì €ì¥ ê²½ë¡œ ì •ì˜
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)  # âœ… `data` í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
RESAMPLED_FILE = os.path.join(DATA_DIR, "data_resampled.csv")

def data_preprocessing():
    print('âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...')


    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    DATA_DIR = os.path.join(BASE_DIR, "data")
    os.makedirs(DATA_DIR, exist_ok=True)  # âœ… data í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±

    # âœ… ì›ë³¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    DATA_PATH = os.path.join(DATA_DIR, "Japan earthquakes 2001 - 2018.csv")

    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {DATA_PATH}\nğŸ“Œ `data/` í´ë”ì— CSV íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")

    # âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    data = pd.read_csv(DATA_PATH, encoding="utf-8")
    print(f"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {DATA_PATH}")
    
    # âœ… ì»¬ëŸ¼ëª… í™•ì¸ ë° ì •ë¦¬
    print(f"âœ… ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ëª…: {data.columns.tolist()}")
    data.columns = data.columns.str.strip()  # ê³µë°± ì œê±°
    print(f"âœ… ì •ë¦¬ëœ ì»¬ëŸ¼ëª…: {data.columns.tolist()}")

    # ê²°ì¸¡ì¹˜ í™•ì¸
    data.isnull().sum()
    data.isnull().mean()*100


    # ê²°ì¸¡ì¹˜ ì‹œê°í™”
    plt.figure(figsize=(20,12))
    sns.heatmap(data.isnull(), cmap='viridis', cbar=False)
    plt.show()


    # 1ï¸âƒ£ timeì„ datetimeìœ¼ë¡œ ë³€í™˜
    data['time'] = pd.to_datetime(data['time'], errors='coerce')
    
    # 2ï¸âƒ£ timeì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì • í›„ ì •ë ¬
    data.set_index('time', inplace=True)
    data.sort_index(inplace=True)


    print(data)


    # ìœ„ë„, ê²½ë„ ê²°ì¸¡ì¹˜ í™•ì¸
    null_lat = data['latitude'].isnull().sum()
    null_long = data['longitude'].isnull().sum()

    print(f'latitude ê²°ì¸¡ì¹˜ ìˆ˜ : {null_lat}')
    print(f'longitude ê²°ì¸¡ì¹˜ ìˆ˜ : {null_long}')


    # ìœ„ë„, ê²½ë„ ë²”ìœ„ í™•ì¸
    min_lat = data['latitude'].min()
    max_lat = data['latitude'].max()

    min_long = data['longitude'].min()
    max_long = data['longitude'].max()

    print(f'ìœ„ë„ ë²”ìœ„ : {min_lat:.2f} ~ {max_lat:.2f}')
    print(f'ê²½ë„ ë²”ìœ„ : {min_long:.2f} ~ {max_long:.2f}')


    # ì§€ì§„ì´ ì¼ì–´ë‚œ ìœ„ì¹˜ë¥¼ ì§€ë„ ìœ„ì— í‘œì‹œí•˜ì—¬ ì‹œê°í™”

    # GeoDataFrameìœ¼ë¡œ ë³€í™˜
    gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data['longitude'], data['latitude']))

    # ì§€ë„ ì„¤ì •
    fig, ax = plt.subplots(figsize=(10, 8))
    gdf.plot(ax=ax, color='blue', markersize=1)  # ë°ì´í„°í”„ë ˆì„ì˜ ì  í‘œì‹œ

    # ë°°ê²½ì— ì§€ë„ íƒ€ì¼ ì¶”ê°€ (OpenStreetMap Mapnik ì‚¬ìš©)
    ctx.add_basemap(ax, crs="EPSG:4326", source=ctx.providers.OpenStreetMap.Mapnik)

    # ì¶• ë²”ìœ„ ì„¤ì • (ì§€ì§„ì´ ì¼ì–´ë‚œ ì§€ì—­ë§Œ í‘œì‹œ)
    ax.set_xlim(min_long, max_long)
    ax.set_ylim(min_lat, max_lat)

    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.title('Earthquake points in Map')
    plt.show()


    # depth, mag, magType ê²°ì¸¡ì¹˜ í™•ì¸

    null_dep = data['depth'].isnull().sum()
    null_mag = data['mag'].isnull().sum()
    null_magType = data['magType'].isnull().sum()

    print(f'depth ê²°ì¸¡ì¹˜ ìˆ˜ : {null_dep}')
    print(f'mag ê²°ì¸¡ì¹˜ ìˆ˜ : {null_mag}')
    print(f'magType ê²°ì¸¡ì¹˜ ìˆ˜ : {null_magType}')


    # depth ê°’ì˜ ë²”ìœ„ í™•ì¸

    depth_min = data['depth'].min()
    depth_max = data['depth'].max()
    print(f'depth ë²”ìœ„ : {depth_min:.2f} ~ {depth_max:.2f}')
    print()

    # ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['depth'], bins=70, color='skyblue', edgecolor='black')
    plt.xlabel('Depth')
    plt.ylabel('Frequency')
    plt.title('Distribution of Depth')
    plt.show()

    # ê° êµ¬ê°„ë³„ ê°’ì˜ ê°œìˆ˜ í™•ì¸
    df_depth = pd.DataFrame(data, columns=['depth'])

    depth_bins = [0, 100, 200, 300, 400, 500, 600, 700]
    depth_labels = ['0 ~ 100', '100 ~ 200', '200 ~ 300', '300 ~ 400', '400 ~ 500', '500 ~ 600', '600 ~ 700']

    df_depth['depth_range'] = pd.cut(df_depth['depth'], bins=depth_bins, labels=depth_labels, right=False)
    depth_range_counts = df_depth['depth_range'].value_counts().sort_index()

    print(depth_range_counts)


    # mag ê°’ì˜ ë²”ìœ„ í™•ì¸

    mag_min = data['mag'].min()
    mag_max = data['mag'].max()
    print(f'mag ë²”ìœ„ : {mag_min:.2f} ~ {mag_max:.2f}')
    print()

    # ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['mag'], bins=50, color='skyblue', edgecolor='black')
    plt.xlabel('Mag')
    plt.ylabel('Frequency')
    plt.title('Distribution of Mag')
    plt.show()

    # ê° êµ¬ê°„ë³„ ê°’ì˜ ê°œìˆ˜ í™•ì¸
    df_mag = pd.DataFrame(data, columns=['mag'])

    mag_bins = [4.5, 5.0, 5.5, 6.0, 7.0, 8.0, 9.0, 10.0]
    mag_labels = ['4.5 ~ 5.0', '5.0 ~ 5.5', '5.5 ~ 6.0', '6.0 ~ 7.0', '7.0 ~ 8.0', '8.0 ~ 9.0', '9.0 ~ 10.0']

    df_mag['mag_range'] = pd.cut(df_mag['mag'], bins=mag_bins, labels=mag_labels, right=False)
    mag_range_counts = df_mag['mag_range'].value_counts().sort_index()

    print(mag_range_counts)


    # magType value í™•ì¸
    print(data['magType'].unique())
    print(data['magType'].value_counts())

    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['magType'], color='skyblue', edgecolor='black')
    plt.xlabel('magType')
    plt.ylabel('Frequency')
    plt.title('Distribution of magType')
    plt.show()


    null_rms = data['rms'].isnull().sum()
    print(f'rms ê²°ì¸¡ì¹˜ ìˆ˜ : {null_rms}')

    null_ratio_rms = data['rms'].isnull().mean()*100
    print(f'rms ê²°ì¸¡ì¹˜ ë¹„ìœ¨ : {null_ratio_rms:.2f}%')


    # rms ê°’ì˜ ë²”ìœ„ í™•ì¸

    rms_min = data['rms'].min()
    rms_max = data['rms'].max()
    print(f'rms ë²”ìœ„ : {rms_min:.2f} ~ {rms_max:.2f}')
    print()

    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['rms'], color='skyblue', edgecolor='black')
    plt.xlabel('rms')
    plt.ylabel('Frequency')
    plt.title('Distribution of RMS')
    plt.show()


    # nst ê²°ì¸¡ì¹˜ í™•ì¸
    null_nst = data['nst'].isnull().sum()
    print(f'nst ê²°ì¸¡ì¹˜ ìˆ˜ : {null_nst}')

    null_ratio_nst = data['nst'].isnull().mean()*100
    print(f'nst ê²°ì¸¡ì¹˜ ë¹„ìœ¨ : {null_ratio_nst:.2f}%')

    # nst ê°’ì˜ ë²”ìœ„ í™•ì¸
    nst_min = data['nst'].min()
    nst_max = data['nst'].max()
    print(f'nst ë²”ìœ„ : {nst_min} ~ {nst_max}')
    print()

    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['nst'], bins = 50, color='skyblue', edgecolor='black')
    plt.xlabel('nst')
    plt.ylabel('Frequency')
    plt.title('Distribution of NST')
    plt.show()


    # magNst ê²°ì¸¡ì¹˜ í™•ì¸
    null_magNst = data['magNst'].isnull().sum()
    print(f'magNst ê²°ì¸¡ì¹˜ ìˆ˜ : {null_magNst}')

    null_ratio_magNst = data['magNst'].isnull().mean()*100
    print(f'magNst ê²°ì¸¡ì¹˜ ë¹„ìœ¨ : {null_ratio_magNst:.2f}%')

    # magNst ê°’ì˜ ë²”ìœ„ í™•ì¸
    magNst_min = data['magNst'].min()
    magNst_max = data['magNst'].max()
    print(f'magNst ë²”ìœ„ : {magNst_min} ~ {magNst_max}')
    print()

    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['magNst'], bins = 50, color='skyblue', edgecolor='black')
    plt.xlabel('magNst')
    plt.ylabel('Frequency')
    plt.title('Distribution of magNst')
    plt.show()


    # gap ê²°ì¸¡ì¹˜ í™•ì¸
    null_gap = data['gap'].isnull().sum()
    print(f'gap ê²°ì¸¡ì¹˜ ìˆ˜ : {null_gap}')

    null_ratio_gap = data['gap'].isnull().mean()*100
    print(f'gap ê²°ì¸¡ì¹˜ ë¹„ìœ¨ : {null_ratio_gap:.2f}%')

    # gap ê°’ì˜ ë²”ìœ„ í™•ì¸
    gap_min = data['gap'].min()
    gap_max = data['gap'].max()
    print(f'gap ë²”ìœ„ : {gap_min} ~ {gap_max}')
    print()

    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['gap'], bins = 50, color='skyblue', edgecolor='black')
    plt.xlabel('gap')
    plt.ylabel('Frequency')
    plt.title('Distribution of gap')
    plt.show()


    # dmin ê²°ì¸¡ì¹˜ í™•ì¸
    null_dmin = data['dmin'].isnull().sum()
    print(f'dmin ê²°ì¸¡ì¹˜ ìˆ˜ : {null_dmin}')

    null_ratio_dmin = data['dmin'].isnull().mean()*100
    print(f'dmin ê²°ì¸¡ì¹˜ ë¹„ìœ¨ : {null_ratio_dmin:.2f}%')

    # dmin ê°’ì˜ ë²”ìœ„ í™•ì¸
    dmin_min = data['dmin'].min()
    dmin_max = data['dmin'].max()
    print(f'dmin ë²”ìœ„ : {dmin_min} ~ {dmin_max}')
    print()

    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['dmin'], bins = 50, color='skyblue', edgecolor='black')
    plt.xlabel('dmin')
    plt.ylabel('Frequency')
    plt.title('Distribution of dmin')
    plt.show()


    # horizontalError ê²°ì¸¡ì¹˜ í™•ì¸
    null_horizontalError = data['horizontalError'].isnull().sum()
    print(f'horizontalError ê²°ì¸¡ì¹˜ ìˆ˜ : {null_horizontalError}')

    null_ratio_horizontalError = data['horizontalError'].isnull().mean()*100
    print(f'horizontalError ê²°ì¸¡ì¹˜ ë¹„ìœ¨ : {null_ratio_horizontalError:.2f}%')

    # horizontalError ê°’ì˜ ë²”ìœ„ í™•ì¸
    horizontalError_min = data['horizontalError'].min()
    horizontalError_max = data['horizontalError'].max()
    print(f'horizontalError ë²”ìœ„ : {horizontalError_min} ~ {horizontalError_max}')
    print()

    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['horizontalError'], bins = 50, color='skyblue', edgecolor='black')
    plt.xlabel('horizontalError')
    plt.ylabel('Frequency')
    plt.title('Distribution of horizontalError')
    plt.show()


    # depthError ê²°ì¸¡ì¹˜ í™•ì¸
    null_depthError = data['depthError'].isnull().sum()
    print(f'depthError ê²°ì¸¡ì¹˜ ìˆ˜ : {null_depthError}')

    null_ratio_depthError = data['depthError'].isnull().mean()*100
    print(f'depthError ê²°ì¸¡ì¹˜ ë¹„ìœ¨ : {null_ratio_depthError:.2f}%')

    # depthError ê°’ì˜ ë²”ìœ„ í™•ì¸
    depthError_min = data['depthError'].min()
    depthError_max = data['depthError'].max()
    print(f'depthError ë²”ìœ„ : {depthError_min} ~ {depthError_max}')
    print()

    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['depthError'], bins = 50, color='skyblue', edgecolor='black')
    plt.xlabel('depthError')
    plt.ylabel('Frequency')
    plt.title('Distribution of depthError')
    plt.show()


    # magError ê²°ì¸¡ì¹˜ í™•ì¸
    null_magError = data['magError'].isnull().sum()
    print(f'magError ê²°ì¸¡ì¹˜ ìˆ˜ : {null_magError}')

    null_ratio_magError = data['magError'].isnull().mean()*100
    print(f'magError ê²°ì¸¡ì¹˜ ë¹„ìœ¨ : {null_ratio_magError:.2f}%')

    # magError ê°’ì˜ ë²”ìœ„ í™•ì¸
    magError_min = data['magError'].min()
    magError_max = data['magError'].max()
    print(f'magError ë²”ìœ„ : {magError_min} ~ {magError_max}')
    print()

    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    plt.hist(data['magError'], bins = 50, color='skyblue', edgecolor='black')
    plt.xlabel('magError')
    plt.ylabel('Frequency')
    plt.title('Distribution of magError')
    plt.show()


    # id, type ê²°ì¸¡ì¹˜ í™•ì¸
    null_id = data['id'].isnull().sum()
    null_type = data['type'].isnull().sum()

    print(f'id ê²°ì¸¡ì¹˜ ìˆ˜ : {null_id}')
    print(f'type ê²°ì¸¡ì¹˜ ìˆ˜ : {null_type}')


    # id value í™•ì¸
    print(data['id'].unique())
    print(data['id'].value_counts())

    # id ê°’ ì¤‘ë³µ ì—¬ë¶€ í™•ì¸
    print(data['id'].duplicated().any())


    # type value í™•ì¸
    print(data['type'].unique())
    print(data['type'].value_counts())


    # place ê²°ì¸¡ì¹˜ í™•ì¸

    null_place = data['place'].isnull().sum()
    print(f'place ê²°ì¸¡ì¹˜ ìˆ˜ : {null_place}')


    # place value í™•ì¸
    print(data['place'].unique())
    print(data['place'].value_counts())


    data['place']


    # updated, status ê²°ì¸¡ì¹˜ í™•ì¸

    null_updated = data['updated'].isnull().sum()
    null_status = data['status'].isnull().sum()

    print(f'updated ê²°ì¸¡ì¹˜ ìˆ˜ : {null_updated}')
    print(f'status ê²°ì¸¡ì¹˜ ìˆ˜ : {null_status}')


    data['updated']


    # status value í™•ì¸
    print(data['status'].unique())
    print(data['status'].value_counts())


    # net, locationSource, magSource ê²°ì¸¡ì¹˜ í™•ì¸

    null_net = data['net'].isnull().sum()
    null_locationSource = data['locationSource'].isnull().sum()
    null_magSource= data['magSource'].isnull().sum()

    print(f'net ê²°ì¸¡ì¹˜ ìˆ˜ : {null_net}')
    print(f'locationSource ê²°ì¸¡ì¹˜ ìˆ˜ : {null_locationSource}')
    print(f'magSource ê²°ì¸¡ì¹˜ ìˆ˜ : {null_magSource}')


    # net value í™•ì¸
    print(data['net'].unique())
    print(data['net'].value_counts())


    # locationSource value í™•ì¸
    print(data['locationSource'].unique())
    print(data['locationSource'].value_counts())


    # magSource value í™•ì¸
    print(data['magSource'].unique())
    print(data['magSource'].value_counts())


    # ìƒê´€ê´€ê³„ë¥¼ êµ¬í•  column ì„ íƒ
    columns_to_corr = ['latitude', 'longitude', 'depth','mag','rms']
    data_to_corr = data[columns_to_corr]

    # ìƒê´€ê´€ê³„ í–‰ë ¬ ìƒì„±
    corr_matrix = data_to_corr.corr()

    # magì™€ì˜ ìƒê´€ê´€ê³„ë§Œ ì¶”ì¶œ
    mag_corr = corr_matrix[['mag']].sort_values(by='mag', ascending=False)

    # íˆíŠ¸ë§µ ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    sns.heatmap(mag_corr, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
    plt.title("Correlation with Magnitude")
    plt.show()


    # ì§€ì§„ ìœ í˜• nuclear explosionì¸ ë°ì´í„° í–‰ ì œê±°
    value_to_remove = 'nuclear explosion'
    data = data[data['type'] != value_to_remove]

    # rms ê°’ì´ ê²°ì¸¡ì¹˜ì¸ í–‰ ì œê±°
    data.dropna(subset=['rms'], axis=0, inplace=True)

    data.info()


    # í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” column ì œê±°
    data.drop(['place', 'horizontalError', 'depthError','magError', 'dmin','magNst','nst','id','locationSource','magSource','status','type', 'updated','net','gap' ], inplace = True, axis=1)

    data.info()


    # Plate ì—´ ì¶”ê°€
    def label_plate(row):
        lat = row['latitude']
        lon = row['longitude']

        # Eurasian Plate
        if 30 <= lat <= 50 and 100 <= lon <= 140:
            return 'Eurasian Plate'
        # North American Plate
        elif 35 <= lat <= 65 and 140 <= lon <= 180:
            return 'North American Plate'
        # Pacific Plate
        elif 0 <= lat <= 60 and 135 <= lon <= 180:
            return 'Pacific Plate'
        # Philippine Sea Plate
        elif 10 <= lat <= 35 and 120 <= lon <= 145:
            return 'Philippine Sea Plate'
        else:
            return 'Other'

    data['Plate'] = data.apply(label_plate, axis=1)


    # ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ˆì‹œ ë°ì´í„°)
    data_df = data[['latitude', 'longitude', 'Plate']]
    df = pd.DataFrame(data_df)

    # ë°ì´í„°í”„ë ˆì„ì„ GeoDataFrameìœ¼ë¡œ ë³€í™˜
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))

    # ì§€ì—­ë³„ ìƒ‰ìƒ ì§€ì •
    colors = {
        'North American Plate': 'red',
        'Pacific Plate': 'blue',
        'Eurasian Plate': 'purple',
        'Philippine Sea Plate': 'orange',
    }

    # ì§€ë„ ì„¤ì • ë° ì‚°ì ë„ ê·¸ë¦¬ê¸°
    fig, ax = plt.subplots(figsize=(10, 6))

    # ì¼ë³¸ ì§€ì—­ë§Œ ì§€ë„ì— í‘œì‹œí•˜ë„ë¡ ì„¤ì •
    japan_bounds = (122, 146, 24, 46)  # ê²½ë„ ìµœì†Œ, ìµœëŒ€ / ìœ„ë„ ìµœì†Œ, ìµœëŒ€
    ax.set_xlim(japan_bounds[0], japan_bounds[1])
    ax.set_ylim(japan_bounds[2], japan_bounds[3])

    # ë°°ê²½ì— ì§€ë„ íƒ€ì¼ ì¶”ê°€ (OpenStreetMap Mapnik ì‚¬ìš©)
    ctx.add_basemap(ax, crs="EPSG:4326", source=ctx.providers.OpenStreetMap.Mapnik)

    # ì‚°ì ë„ ì¶”ê°€
    for region, group in gdf.groupby('Plate'):
        ax.scatter(group.geometry.x, group.geometry.y, label=region, color=colors[region])

    # ê·¸ë˜í”„ ì„¤ì •
    ax.set_title('ì¼ë³¸ ì§€ë„ ìœ„ì— ìœ„ë„ ë° ê²½ë„ ë¶„í¬')
    ax.set_xlabel('ê²½ë„')
    ax.set_ylabel('ìœ„ë„')
    ax.legend()
    plt.show()


    # Region ì—´ ì¶”ê°€
    def label_japan_region2(row):
        lat = row['latitude']  # ìœ„ë„
        lon = row['longitude']  # ê²½ë„

        # í™‹ì¹´ì´ë„ (Hokkaido)
        if 41 <= lat <= 45.5 and 139 <= lon <= 145.5:
            return 'Hokkaido'
        # ê°„í†  (Kanto)
        elif 34 <= lat <= 37.5 and 138 <= lon <= 141:
            return 'Kanto'
        # ì£¼ë¶€ (Chubu)
        elif 33 <= lat <= 38 and 136 <= lon <= 138.5:
            return 'Chubu'
        # ê°„ì‚¬ì´ (Kansai)
        elif 32.5 <= lat <= 36 and 135 <= lon <= 136.5:
            return 'Kansai'
        # ì‹œì½”ì¿  (Shikoku)
        elif 33 <= lat <= 34.5 and 132.5 <= lon <= 135:
            return 'Shikoku'
        # ê·œìŠˆ (Kyushu)
        elif 29 <= lat <= 34 and 129 <= lon <= 132.5:
            return 'Kyushu'
        # ì˜¤í‚¤ë‚˜ì™€ (Okinawa)
        elif 24 <= lat <= 28 and 123 <= lon <= 130:
            return 'Okinawa'
        # ë„í˜¸ì¿  (Tohoku)
        elif 37 <= lat <= 41.5 and 139 <= lon <= 142.5:
            return 'Tohoku'
        # ì£¼ê³ ì¿  (Chugoku)
        elif 34 <= lat <= 36 and 131 <= lon <= 134.5:
            return 'Chugoku'
        # í‚¨í‚¤ (Kinki)
        elif 34 <= lat <= 35 and 135 <= lon <= 136:
            return 'Kinki'
        else:
            return 'Other'

    data['Region'] = data.apply(label_japan_region2, axis=1)




    # ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ˆì‹œ ë°ì´í„°)
    data_df = data[['latitude', 'longitude', 'Region']]
    df = pd.DataFrame(data_df)

    # ë°ì´í„°í”„ë ˆì„ì„ GeoDataFrameìœ¼ë¡œ ë³€í™˜
    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))

    # ì§€ì—­ë³„ ìƒ‰ìƒ ì§€ì •
    colors = {
        'Hokkaido': 'red',
        'Tohoku': 'blue',
        'Kanto': 'purple',
        'Chubu': 'orange',
        'Kinki': 'pink',
        'Kansai': 'cyan',
        'Chugoku': 'yellow',
        'Shikoku': 'brown',
        'Kyushu': 'magenta',
        'Okinawa': 'black',
        'Other': 'gray'
    }

    # ì§€ë„ ì„¤ì • ë° ì‚°ì ë„ ê·¸ë¦¬ê¸°
    fig, ax = plt.subplots(figsize=(10, 6))

    # ì¼ë³¸ ì§€ì—­ë§Œ ì§€ë„ì— í‘œì‹œí•˜ë„ë¡ ì„¤ì •
    japan_bounds = (122, 146, 24, 46)  # ê²½ë„ ìµœì†Œ, ìµœëŒ€ / ìœ„ë„ ìµœì†Œ, ìµœëŒ€
    ax.set_xlim(japan_bounds[0], japan_bounds[1])
    ax.set_ylim(japan_bounds[2], japan_bounds[3])

    # ë°°ê²½ì— ì§€ë„ íƒ€ì¼ ì¶”ê°€ (OpenStreetMap Mapnik ì‚¬ìš©)
    ctx.add_basemap(ax, crs="EPSG:4326", source=ctx.providers.OpenStreetMap.Mapnik)

    # ì‚°ì ë„ ì¶”ê°€
    for region, group in gdf.groupby('Region'):
        ax.scatter(group.geometry.x, group.geometry.y, label=region, color=colors[region])

    # ê·¸ë˜í”„ ì„¤ì •
    ax.set_title('ì¼ë³¸ ì§€ë„ ìœ„ì— ìœ„ë„ ë° ê²½ë„ ë¶„í¬')
    ax.set_xlabel('ê²½ë„')
    ax.set_ylabel('ìœ„ë„')
    ax.legend()
    plt.show()


    # ìœ¡ì§€ì™€ ë§ì´ ë–¨ì–´ì§„ ìœ„ì¹˜ì—ì„œ ë°œìƒí•œ ë°ì´í„° ì œê±°
    value_to_remove = 'Other'
    data = data[data['Region'] != value_to_remove]
    data.info()


    # ì •ìƒì„± ê²€ì¦
    result = adfuller(data['mag'])
    print('ADF Statistic: %f' % result[0])
    print('p-value: %f' % result[1])
    
    print(type(data.index))
    print(data.index[:5])

    # ë²”ì£¼í˜• ë°ì´í„° ì›-í•« ì¸ì½”ë”©
    data = pd.get_dummies(data,  columns=['magType', 'Region', 'Plate']).astype(int)
    data.info()
    

    # ë°ì´í„°ë¥¼ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ë§ì¶¤
    data.index = pd.to_datetime(data.index)
    data.index = data.index.round('h')
    print("ë°ì´í„°í”„ë ˆì„ ì¸ë±ìŠ¤ ê³ ìœ ê°’ ê°œìˆ˜:", len(data.index.unique()))
    print("ì¸ë±ìŠ¤ ê°’ ìƒ˜í”Œ:", data.index[:5])
    

    # 5ï¸âƒ£ ë°ì´í„° ë¦¬ìƒ˜í”Œë§ (1ì‹œê°„ ë‹¨ìœ„)
    data_resampled = data.resample('h').mean()
    data_resampled = data_resampled.interpolate(method='linear')
    data_resampled.reset_index(inplace=True)
    

    # âœ… ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
    if data_resampled.empty:
        raise ValueError("ğŸš¨ [data_preprocessing.py] ë°ì´í„° ë¦¬ìƒ˜í”Œë§ í›„ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. `data`ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # âœ… ë°ì´í„° ì •ë³´ ì¶œë ¥
    print("âœ… [data_preprocessing.py] ë°ì´í„° ë¦¬ìƒ˜í”Œë§ ì™„ë£Œ:")
    print(data_resampled.info())
    


    # ë°ì´í„° ì €ì¥ ê²½ë¡œ
    DATA_FILE = os.path.join(DATA_DIR, "data.csv")

    # âœ… ë°ì´í„° ì €ì¥
    data_resampled = data_resampled.fillna(0)  # ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì±„ì›€
    data_resampled.to_csv(RESAMPLED_FILE, index=False, date_format="%Y-%m-%d %H:%M:%S")
    
    print("ë¦¬ìƒ˜í”Œë§ ì „ ë°ì´í„° ê°œìˆ˜:", len(data))
    print("ë¦¬ìƒ˜í”Œë§ í›„ ë°ì´í„° ê°œìˆ˜:", len(data_resampled))    
    
    

   # âœ… íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if os.path.exists(RESAMPLED_FILE):
        print(f"âœ… [data_preprocessing.py] ë°ì´í„° ì €ì¥ ì™„ë£Œ: {RESAMPLED_FILE}")
    else:
        raise FileNotFoundError(f"ğŸš¨ [data_preprocessing.py] ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {RESAMPLED_FILE}")
    
if __name__ == '__main__':
    data_preprocessing()
